model:
  vocab_size: 119547 # tokenizer.vocab_size
  embed_size: 128 # 256
  hidden_size: 96
  num_layers: 4
  dropout: 0.4

training:
  batch_size: 32
  num_epochs: 30
  learning_rate: 0.001
  weight_decay: 0 # 0.000001
  patience: 3
  min_delta: 0.01

data:
  max_seq_len: 768

general:
  seed: 42