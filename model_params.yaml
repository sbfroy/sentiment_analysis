# with optunas best_params_v3

model:
  vocab_size: 119547 # tokenizer.vocab_size
  embed_size: 128
  hidden_size: 128
  num_layers: 3
  dropout: 0.35

training:
  batch_size: 32
  num_epochs: 40
  learning_rate: 0.00016488555876672772
  weight_decay: 2.0921420132719923e-05
  patience: 3
  min_delta: 0.01
  gradient_clip_val: 0.13747699699142135

data:
  max_seq_len: 704

general:
  seed: 42